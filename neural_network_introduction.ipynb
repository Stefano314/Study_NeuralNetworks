{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Neural Networks Introduction:**\n",
    "## *\"practical guide on how to develop neural networks for solving basic problems\"*\n",
    "\n",
    "<br/><br/>\n",
    "## **What is a neural network?**\n",
    "To put it simply, a neural network is a function $f = f(\\vec{x})$ such that:\n",
    "\\begin{align*}\n",
    "  f \\colon \\Sigma &\\to \\Omega\\\\\n",
    "  \\vec{x} &\\mapsto \\vec{y}.\n",
    "\\end{align*}\n",
    "where $\\vec{x}\\in\\Sigma$ is the input vector - *or matrix* - and it gives back an output $\\vec{y}\\in\\Omega$. The domains $\\Omega$ and $\\Sigma$ are usually real domains, for instance $\\Omega = \\mathbb{R}^n$ and $\\Sigma = \\mathbb{R}^m$; if the input is a **matrix** or even a **tensor**- that can happen when dealing with images - we might want to *flatten* the $N\\times N$ matrix into an $N\\cdot N$ vector, or we can perform specific actions in order to reduce the matrix into something more practical, as we will see in convolutional neural networks.\n",
    "The function $f$ manipulates the input $\\vec{x}$ according to the structure of the neural network. This function is usually the combination of intermediate functions $\\{g_i\\}_1^l$, where $l-1$ is the number of so called *\"hidden layers\"*, while the last function $g_l$ produces the *output*. Therefore we can rewrite $f$ as:\n",
    "\\begin{equation*}\n",
    "f = g_l\\circ g_{l-1} \\circ \\, ...\\, \\circ g_1 \\,.\n",
    "\\end{equation*}\n",
    "Each function $g_i$ is defined between two vector spaces of arbitrary dimension, depending on the *input vector* dimensionality of that layer and the wanted output one.\n",
    "\n",
    "The strength of a neural network is to be found in two aspect, one theoretical and the other more practical: the first one lies in the definition of each $g_i$ and the second is merely in the computational power of nowdays computers. In fact, each function $g_i$ is defined as:\n",
    "\\begin{equation*}\n",
    "g_i(\\vec{z}_i) = \\vec{z}_{i+1}\\; ,\n",
    "\\end{equation*}\n",
    "where the $j$-th component of the vector $\\vec{z}_{i+1}$ is given by:\n",
    "\\begin{equation*}\n",
    "z^{(j)}_{i+1} = a_{i+1}(\\vec{w}_{i+1}^T\\cdot \\vec{z}_i + b_{i+1})\\; .\n",
    "\\end{equation*}\n",
    "This seems quite articulated - in fact it is - but only from a \"too many things to do\" point of view, which is not a problem for a computer. The concept in reality, is pretty straight forward. The function $a_{i+1}(v)$ is called **activation function** relative to the layer $i+1$ and it is a scalar function, usually something like this:\n",
    "\\begin{equation*}\n",
    "a(v) = \\dfrac{1}{1+\\exp{(-v)}} \\, , \\; a(v) = \\tanh{(v)} \\, , \\; a(v) = \\max (0,v)\\;, \\, ...\n",
    "\\end{equation*}\n",
    "where $v$ is again a scalar defined as:\n",
    "\\begin{equation*}\n",
    "v = \\vec{w}_{i+1}^T\\cdot \\vec{z}_i + b_{i+1}\\; .\n",
    "\\end{equation*}\n",
    "The input of $a$ is therefore the *scalar product* plus a bias $b_{i+1}$ of the current layer input vector $\\vec{z}_i$ and a **weight vector** $\\vec{w}_{i+1}$ relative to the $i+1$-th layer. We may have many definitions of $a$, even among different layers, according to the neural network purposes. The activation function is crucial in a neural network to fit **non-linear data**; it can also be linear, meaning that $a(v)=k\\cdot v + b$, with $k$ a multiplicative constant, but in this way we are simply doing a normal **linear regression**.\n",
    "\n",
    "This is basically it, in fact now the only thing that we need to do is to define a function $S(\\vec{w},\\vec{b})$ which is a function of all the weights and biases in the neural network structure - which are a lot - and we want to minimize this function. Of course, to do this we need a so called **Loss function** $L(\\vec{y}, \\vec{\\hat{y}})$ that represents the accuracy of our neural network processed vector $\\vec{y}$ - also called *predicted output* - compared to a test one, usually a real one, $\\vec{\\hat{y}}$. This is an iterative procedure, so you *feed* the neural network with the current output vector as new input, you compare the score with the real results $\\vec{\\hat{y}}$ and so on until the accuracy is satisfying. Careful not to overfit, if that is not what you want!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Types of Training**\n",
    "There are four types of machine learning training, namely **supervised learning**, **unsupervised learning**, **semi-supervised** and **reinforcement learning**. We will now briefly go through all these types, but pointing out the fundamentals differences between them.\n",
    "\n",
    "### **1. Supervised Learning**\n",
    "This is a widely used type of machine learning procedure, which is based on having **lables** that categorize the type of data used. This emulates - to some extent - the human learning procedure: we see an object, we know from external inputs in which category it belongs and then we are able to recognize a similar object within the same category. And exactly as in humans, supervised learning can lead to mistakes, for instance if the object is far and we are not able to properly recognize its key features, like its shape, color, etc..\n",
    "\n",
    "The general setup for this type of learing is the following:\n",
    "```(training_dataset, training_labels), (testing_dataset, testing_labels), (validation_dataset, validation_labels)```.\n",
    "The training section is used to update the **weights** of the links in the neural network, the testing section is a metric to *evaluate* the performance of the model on unseen data. The validation section may not appear, and is responsible for the **hyperparameters** tuning; the hyperparameters are those elements related to the structure of the neural network, for instance the *number of nodes* in each layer, the dataset *batch size*, the *learning rate*, etc.. The appearance these sections is the same, the important requirement is the absence of duplicates among them.\n",
    "\n",
    "### **2. Unsupervised Learning**\n",
    "In this case, as the name suggests, we do not have any labels as the model's purpose is not the one of making predictions. In humans we do also have this type of learning, in particular all the times that we face something we don't know and there's no one that describes it. Think about swimming *without* an instructor: we might have serius difficulties early on, but we might be able to stay afloat and move with a bit of practice. We don't have to make predictions on anything, we just want to learn a **pattern** - arm/legs movements - that allows us to complete a task - swimming.\n",
    "In machine learning we do this with some techinques like **clustering** and **segmentations**.\n",
    "\n",
    "The typical dataset in this case is just ```(training_set)```, which is quite an issue when trying to evaluate the performance of the model. There are techniques to do it, but we won't discuss them now.\n",
    "\n",
    "### **3. Semi-Supervised Learning**\n",
    "This technique merges the ideas of the ones above; in particular, we train the model on a smaller dataset with labels and then we keep processing a larger amount of unlabeled data in order to identify the sample space structure. This should produce a quite flexible model that categorizes objects with a wider margin. Usually, this techinque is a **necessity**, because we might not have enough labeled data, which is very common.\n",
    "\n",
    "The typical data structure is similar to the supervised one:\n",
    "```(training_dataset_l, training_labels_l), (testing_dataset_u), (testing_dataset_l, testing_labels_l), (validation_dataset_l, validation_labels_l)```,\n",
    "where in this case the testing dataset is devided in also in an *unlabeled* one. The 'u' and 'l' stand for \"unlabeled\" and \"labeled\"; it is quite rare to have enough labeled data to use them also in a validation dataset, but it can be there.\n",
    "\n",
    "### **4. Reinforcement Learning**\n",
    "The final learning category is somewhat similar as concept to the *unsupervised* one. Even in this case we don't have lables and we want to find some patterns in our sample space that suits our needs. The main difference here is that we have a **goal** in mind, for instance completing a task. Going back to the swimming example, if we do have an *instructor* he can help reaching the right technique by telling us if we are doing good or bad. It's important to notice that the example fits up to a certain point: in real life the instructor knows the right technique, instead in the reinforcement learning the instructor simply doesn't, but he can say if the one that we are using is *more optimal* than another one.\n",
    "\n",
    "In machine learning this is achieved by using **rewards**, or in other words a quantity whose cumulative over time should be maximized. The idea is *\"exploration vs. exploitation trade-off\"*, where we program a model that is prone to try new approaches - exploration - while keeping the current knowledge THAT IS WORKING in mind - exploitation. In general, an action would produce a given reward over time; there is a modification to that action, or even a different one, that increases the reward, the model will adapt in order to implement that action in its exploitation repertoire.\n",
    "This technique fits perfectly any *game AI* that has to do some actions, for instance try to beat the player in a car race.\n",
    "\n",
    "In this case the dataset is quite different than the others, we will discuss it better in ```reinforcement_learning.ipynb```."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  **Neural Networks architectures**\n",
    "There are many neural network architectures that serve very specific needs. The most common ones are **Liner**, **Convolutional**, **Recurrent** and **Long Short-Term Memory**, which we are going to describe in the respective files. The general structure is more or less the same for every one, with few but crucial changes that produce very different behaviors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "9d4ed44c6a2a13ca6ab9393ffa30cf54197e027a07f139da5103fcdd4e6439bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
